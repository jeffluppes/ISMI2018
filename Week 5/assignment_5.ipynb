{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teaching Assistants:\n",
    "- Wouter Bulten: wouter.bulten@radboudumc.nl\n",
    "- David Tellez Martin: David.TellezMartin@radboudumc.nl\n",
    "\n",
    "Please submit your notebook via grand-challenge.org (https://ismi-cifar.grand-challenge.org/introduction/).\n",
    "Submit a notebook **WITH ALL CELLS EXECUTED!!!**\n",
    "\n",
    "* Groups: You should work in pairs or alone\n",
    "* Deadline for this assignment: \n",
    " * Monday (March 12th) until midnight\n",
    " * 5 points (maximum grade = 100 points) penalization per day after deadline\n",
    "* Submit your **fully executed** notebook to the grand-challenge.org platform\n",
    "\n",
    "### Students\n",
    "Please fill in this cell with your name and e-mail address. This information will be used to grade your assignment.\n",
    "\n",
    "* <font color=#8b0000> Brian Westerweel, email address: b.westerweel@student.ru.nl</font>\n",
    "* <font color=#8b0000> Jeffrey Luppes, email address: j.luppes@student.ru.nl</font>\n",
    "* <font color=#8b0000> Gijs van der Meijde, email address: G.vanderMeijde@student.ru.nl</font>\n",
    "\n",
    "We are \"Team whiskey and Ritalin\" on this weeks leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Image classification with convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/cifar_10.jpg\" alt=\"CIFAR10\" align=\"right\" width=\"450\">\n",
    "\n",
    "In this assignment, we are going to build, train and validate **convolutional neural networks** (ConvNets).\n",
    "For this purpose, we will use data from the publicly available CIFAR10 dataset.\n",
    "CIFAR10 is a dataset commonly used in the community of computer vision and machine learning to benchmark new algorithms and network architectures.\n",
    "\n",
    "CIFAR10 is a dataset that contains (small) RGB images of 32x32 px of ten different classes:\n",
    "    * airplane\t\t\t\t\t\t\t\t\t\t\n",
    "    * automobile\t\t\t\t\t\t\t\t\t\t\n",
    "    * bird\t\t\t\t\t\t\t\t\t\t\n",
    "    * cat\t\t\t\t\t\t\t\t\t\t\n",
    "    * deer\t\t\t\t\t\t\t\t\t\t\n",
    "    * dog\t\t\t\t\t\t\t\t\t\t\n",
    "    * frog\t\t\t\t\t\t\t\t\t\t\n",
    "    * horse\t\t\t\t\t\t\t\t\t\t\n",
    "    * ship\t\t\t\t\t\t\t\t\t\t\n",
    "    * truck\n",
    "More details can be found at this link: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "We recently searched for the state-of-the-art result on this dataset, and apparently an error of **2.31** on the test set (accuracy = 97.69) has been recently reached. The approach is described in this paper: https://openreview.net/pdf?id=S1NHaMW0b\n",
    "\n",
    "In this assignment, you will probably reach an accuracy between 60% and 70%, which is perfectly fine!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "We will train our ConvNet with images from CIFAR10, a dataset of 60,000 colour images of 32x32 pixels in 10 classes. The downloaded training samples come split into 5 batches of 10,000 samples each, which is useful to do cross-validation for example. In this assignment, you will have to decide how to best split the dataset into training and validation sets. A separate test set is provided in CIFAR10, which is the same set used by other researchers to benchmark their methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "We define 10 tasks in this assignment.\n",
    "Although 10 tasks sound like a lot of work, you will see that (1) they are highly repetitive (meaning, you will do a lot of copy-paste across cells in the notebook), (2) you can reuse most of the things you did last week.\n",
    "Like we mentioned in the lecture this week, given the experience you have gained by defining datasets and training neural networks, training convolutional neural networks is just a natural step towards using a different architectures (and some additional tricks and tools).\n",
    "\n",
    "---\n",
    "\n",
    "The task that we propose are the following:\n",
    "\n",
    "### Task 1. Split data and build convolutional networks (30 points)\n",
    "In this task, you will first define your training and validation set, then you will build the architecture of convolutional networks.\n",
    "\n",
    "### Task 2. Train convolutional networks (20 points)\n",
    "In this task, you will train your first model using CIFAR10, apply the trained model to the test set, and submit the results to grand-challenge. In order to define training functions and parameteres, you can reuse a lot of code developed last week.\n",
    "\n",
    "### Task 3. Add dropout layer(s) (5 points)\n",
    "In this task, you will modify the architecture of your network by adding dropout, which is implemented in Keras in the form of a layer. After that, you will repeat the training procedure and compare the results with the ones of the *plain* network.\n",
    "\n",
    "### Task 4. Add batch normalization (5 points)\n",
    "In this task, you will do something similar to task 3, but now adding batch normalization.\n",
    "You will repeat the experiment and compare the performance with previous architectures.\n",
    "\n",
    "### Task 5. Try different initialization strategies (5 points)\n",
    "We have seen that at least a couple of initilization strategies are known in the literature for (convolutional) neural networks.\n",
    "Several strategies are implemented in the Keras library.\n",
    "Try some of them and report the results.\n",
    "\n",
    "### Task 6. Try different nonlinearities (5 points)\n",
    "The same for nonlinearities, we have seen that ```ReLU``` is some kind of default choice for ConvNets, but other strategies exists. Do experiments, report the results and compare with previous approaches.\n",
    "\n",
    "### Task 7. Add L2 regularization (5 points)\n",
    "Modify the loss function to use L2 regularization.\n",
    "Again, run experiments and report results.\n",
    "\n",
    "### Task 8. Add data augmentation (15 points + 10 bonus points)\n",
    "Think of possible ways you can augment the (training) data.\n",
    "You can build a new (bigger) training set, or implement some kind of data augmentation *on-the-fly*, where some patches in the mini-batch are randomly selected and augmented with a (random) operation. Think of transformations that make sense in the context of classification of natural images.\n",
    "\n",
    "### Task 9. Try different architecture (10 points)\n",
    "You can try to improve the performance by modifying the architecture, using more layers, or wider layers (same number of layers but more filters, which means more parameters). Use all the tools you have investigated so far, the optimal combination of the options you have tried in previous tasks. The goal is to get high accuracy on the validation (and therefore on the test) set!\n",
    "\n",
    "### Task 10 (optional). Monitor the training procedure (max 10 points)\n",
    "Finally, an optional task is to implement some tools to monitor the training procedure.\n",
    "Examples are the analysis of statistics of activations, or visualizing the filters learned.\n",
    "If done during training, visualizing filter will also nicely show how the network refines random parameters to come up with meaningful filters (especially in the first layer).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "As done in previous assignments, in this notebook we provide some parts of code implemented.\n",
    "Some other parts are not implemented, but we define the variables that will be used in functions, to help you in the development of the assignment.\n",
    "Things that have been declared but not implemented are assigned a **None** value.\n",
    "That is the part that you have to implement.\n",
    "This means that every time you see **None**, it means that something is missing and you have to implement it.\n",
    "\n",
    "## Reporting your results\n",
    "\n",
    "When you are done with the assignments you will have to hand in this notebook. Make sure that for each assignment you output a summary of the network architecture and a plot of the loss/accuracy curve during training. This output will be used to grade your assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (20, 12)\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from IPython import display\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, models, activations\n",
    "#from PIL import Image\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get to know your data: Load data and define datasets\n",
    "\n",
    "CIFAR10 contains 5 batches that can be used for training/validation, and one batch that consists of the test set.\n",
    "In order to train your network, you will have to define a training set and a validation set. Do not use the test set as training data, and do not use any knowledge on the labels of the test set (being a publicly available dataset, we cannot avoid exposing the labels of the test set).\n",
    "\n",
    "\n",
    "Think of the best way to split data into training and validation set. Note that the format that layers in convolutional networks like (at least in the Keras/Tensorflow libraries that we are using), is as follows:\n",
    "\n",
    "    (n_samples, rows, cols, n_channels)\n",
    "\n",
    "This means that each training (but also validation and test) sample needs to have four dimensions. This kind of structure (multi-dimensional array), is called a **tensor**. In practice, this format is also convenient because the first index of the tensor refers to the sample itself, so we can use:\n",
    "\n",
    "    tensor[i]\n",
    "    \n",
    "to extract the i-th example.\n",
    "\n",
    "During training, several samples will be used to update the parameters of a network. In the case of CIFAR10, if we use M samples per mini-batch, the shape of the mini-batch data is:\n",
    "\n",
    "    (M, 32, 32, 3)\n",
    "\n",
    "Make sure data is organized in this way, for the training, validation and test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download CIFAR10 data\n",
    "The following cell will download the 5 batches of the CIFAR10 training dataset as well as the test set to your disk and load it in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.data_utils import get_file\n",
    "origin_url = 'https://surfdrive.surf.nl/files/index.php/s/{key}/download'\n",
    "\n",
    "# Dictionary mapping file names to download keys\n",
    "files = {\n",
    "    'cifar10-batch-1': '4Nee36XgbYjp3N6',\n",
    "    'cifar10-batch-2': 'LVx85dLceKI5toF', \n",
    "    'cifar10-batch-3': '4FoFmeKyXa5sYr0',\n",
    "    'cifar10-batch-4': 'a4o2RatC0Fa3Exb',\n",
    "    'cifar10-batch-5': 'eqBJRWdkRGk025k',\n",
    "    'cifar10-test': 'nEf9Z4eg7iGmWGU'\n",
    "}\n",
    "\n",
    "# Download files to local drive (only downloads the files once)\n",
    "local_files = [{'name': name, 'file': get_file(name, origin=origin_url.format(key=key))} for name, key in files.items()]\n",
    "\n",
    "# Load all batches in memory\n",
    "batches = {lf['name']: pickle.load(open(lf['file'], 'rb'), encoding='latin1') for lf in local_files}\n",
    "\n",
    "# Convert data to floats and reshape to correct format\n",
    "# This is specific for CIFAR10\n",
    "for batch in batches.values():\n",
    "    # Images were originally used as (CxWxH), convert to (WxHxC)\n",
    "    batch['data'] = (batch['data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1) / 255.).astype(\"float32\")\n",
    "\n",
    "data_size_in=(32, 32, 3)\n",
    "n_classes=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CIFAR dataset is now loaded in memory in the form of a Python dictionary. To get to know your data, check what are the keys of such a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batches.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you define your datasets, it is useful to check the distribution of labels across batches of CIFAR10, in case some batches have skewed distributions of labels.\n",
    "In order to do that, you can use visualize the histogram of labels using the function ```hist()``` of the matplotlib library:\n",
    "\n",
    "    plt.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 3))\n",
    "plt.subplot(1,5,1); plt.hist(batches['cifar10-batch-1']['labels']); plt.title('batch_1')\n",
    "plt.subplot(1,5,2); plt.hist(batches['cifar10-batch-2']['labels']); plt.title('batch_2')\n",
    "plt.subplot(1,5,3); plt.hist(batches['cifar10-batch-3']['labels']); plt.title('batch_3')\n",
    "plt.subplot(1,5,4); plt.hist(batches['cifar10-batch-4']['labels']); plt.title('batch_4')\n",
    "plt.subplot(1,5,5); plt.hist(batches['cifar10-batch-5']['labels']); plt.title('batch_5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question\n",
    "Do you think that you have to organize batches in a specific way in order to make a training and a validation set, and would this affect the performance of your network significantly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The batches are pretty balanced so in this case the effect would be negligible.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement now a function ```load_data``` that builds and returns the training and validation datasets. You will be using this function in your experiments later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Replace None with your code ###\n",
    "# define training, validation and test sets\n",
    "def load_data():\n",
    "    \n",
    "    # make training set\n",
    "    train_x = np.zeros((40000, 32, 32, 3)).astype(\"float32\")\n",
    "    \n",
    "    ### YOUR CODE HERE ###\n",
    "    val_batch = 'cifar10-batch-5'\n",
    "    train_batches = ['cifar10-batch-1', 'cifar10-batch-2', 'cifar10-batch-3', 'cifar10-batch-4']\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    for batch in train_batches:\n",
    "        for i in range(0, len(batches[batch]['data'])):\n",
    "            train_data.append(batches[batch]['data'][i])\n",
    "            train_labels.append(batches[batch]['labels'][i])\n",
    "        \n",
    "    val_data = batches[val_batch]['data']\n",
    "    val_labels = batches[val_batch]['labels']\n",
    "    \n",
    "    for i in range(0, len(train_data)):\n",
    "        train_x[i,:,:,:] = train_data[i] \n",
    "    \n",
    "    # ADD DATA TO THE VARIABLE x_train\n",
    "    \n",
    "    train_y = train_labels\n",
    "        \n",
    "    # make validation set\n",
    "    val_x = val_data\n",
    "    val_y = val_labels\n",
    "\n",
    "    # make test set\n",
    "    test_x = batches['cifar10-test']['data']\n",
    "    \n",
    "    # load labels CIFAR10\n",
    "    label_to_names = {0: 'airplane', 1: 'automobile', 2: 'bird', 3: 'cat', 4: 'deer', 5: 'dog', 6: 'frog', 7: 'horse', 8: 'ship', 9: 'truck'}\n",
    "\n",
    "    print (\"-----------------------------------------------------\")\n",
    "    print (label_to_names)\n",
    "    print (\"-----------------------------------------------------\")\n",
    "    print (\"> shape training set tensor: {}\".format(train_x.shape))\n",
    "    print (\"> length training labels: {}\".format(len(train_y)))\n",
    "    print (\"-----------------------------------------------------\")\n",
    "    print (\"> shape validation set tensor: {}\".format(val_x.shape))\n",
    "    print (\"> length training labels: {}\".format(len(val_y)))\n",
    "    print (\"-----------------------------------------------------\")\n",
    "    print (\"> shape test set tensor: {}\".format(test_x.shape))\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y, test_x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: build a simple convolutional network\n",
    "\n",
    "In this task you create the definition of your convolutional network. You can use parts of the code of  last week as an example. If you want you can check out the **functional API** of Keras; this API enables you to make more advanced models and is more flexible to use. A guide can be found here: https://keras.io/getting-started/functional-api-guide/\n",
    "\n",
    "Example of sequential API:\n",
    "```python\n",
    "from keras import models, layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, input_dim=784), activation='relu')\n",
    "model.add(layers.Dense(32, activation='relu')\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(...)\n",
    "```\n",
    "\n",
    "Example of same model using the functional API (**recommended**):\n",
    "\n",
    "```python\n",
    "from keras import models, layers\n",
    "\n",
    "inputs = layers.Input(shape=(784,))\n",
    "x = layers.Dense(32, activation='relu')(inputs)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "predictions = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(...)\n",
    "```\n",
    "\n",
    "The functional API can look a bit more complex at start in comparision to the sequential API. However, it gives you access to each individual layer and lets you more easily build up your network. The only difference between the two APIs is the way you build up your network, all other parts (training, applying a network, etc.) are the same.\n",
    "\n",
    "For the assignments it does not matter which API you choose, **you are free to choose the sequential or functional API**. For the final assignment of the course it can be usefull to get some experience with the functional API.\n",
    "\n",
    "### Network\n",
    "Define your network builder function. \n",
    "We build a convolutional network that contains:\n",
    "\n",
    "1. input layer\n",
    "2. convolutional layer\n",
    "3. max pooling layer\n",
    "4. convolutional layer\n",
    "5. max pooling layer\n",
    "6. fully-connected layer(s)\n",
    "7. soft-max layer\n",
    "\n",
    "### Hint\n",
    "\n",
    "1. Select the number of convolutional and max pooling layers and choose the filter size so that the input image is shrinked to 5x5 before the fully connected layers.\n",
    "2. Use at least one fully connected layer between the last convolutional layer and the output layer (which is the fully connected layer with softmax nonlinearity).\n",
    "3. You can reuse the code you used last week to build a fully-connected network, just remember that this time you have to use convolutional layers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your network builder function. You can assume the (32, 32, 3) input size.\n",
    "#\n",
    "from keras.models import Model\n",
    "def build_neural_network(data_size_in, n_classes):\n",
    "        \n",
    "    '''  This network scored 68.97% (rank 5 at the time) on Grand Challenge (300 epochs, 128 batch size\n",
    "    '''\n",
    "    \n",
    "    inputs = layers.Input(shape=data_size_in, name='input')\n",
    "    conv1 = layers.Conv2D(32, (3,3), activation='relu')(inputs)\n",
    "    max1 = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(conv1)\n",
    "    conv2 = layers.Conv2D(32, (3,3), activation='relu', padding='same')(max1)\n",
    "    max2 = layers.MaxPooling2D(pool_size=(3, 3), strides=(3, 3))(conv2)\n",
    "    fulcon1 = layers.Dense(80, activation='relu')(max2)\n",
    "    flatten = layers.Flatten()(fulcon1)\n",
    "    softmax = layers.Dense(n_classes, activation='softmax')(flatten)\n",
    "\n",
    "    network = Model(inputs=inputs, output=softmax)\n",
    "    \n",
    "    print(network.summary())\n",
    "    \n",
    "#     return model\n",
    "    return network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function & Optimization Algorithm\n",
    "\n",
    "Now that the architecture is defined, we have to specify the two other components of our learning framework, namely the loss function and the optimization algorithm.\n",
    "\n",
    "Once you have defined these two components, you will have to **compile** the network that you have defined with the function ```build_neural_network```, using the function ```network.compile()```.\n",
    "\n",
    "The ```network.compile()``` function requires the following input parameters:\n",
    "1. loss function -> the loss function\n",
    "2. optimizer -> the optimization algorithm\n",
    "3. metrics -> the performance parameters you want to compute\n",
    "\n",
    "You can find information about how to use the ```compile()``` function at this page: https://keras.io/getting-started/sequential-model-guide/.\n",
    "\n",
    "### Loss\n",
    "We have to define a function that, given the network, gets the predicted probability for a given input sample.\n",
    "Since we are dealing with a multi-class classification problem, **categorical cross-entropy** seems a reasonable choice.\n",
    "\n",
    "### Optimization algorithm\n",
    "We also have to specify how we want to train our model. In our case, we will use \"Stochastic Gradient Descent\". As we have seen in the lecture this week, gradient descent algorithms need a **learning rate**, which indicates how much we step in the (opposite) direction of the gradient. We have also seen that strategy to adapt the learning rate during training are possible, but for the moment we just define a fixed learning rate. Pick a value and see what happens, you can optimize this later.\n",
    "\n",
    "### Metrics\n",
    "Since we are developing a classifier for a multi-class problem, the accuracy seems like a reasonable choice.\n",
    "\n",
    "In the end, you need to compile your network with your settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replace None with your code ###\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "loss = 'categorical_crossentropy' # define the (string) loss function\n",
    "learning_rate = 0.01 # pick a value for your learning rate\n",
    "sgd = SGD(learning_rate) # define Stochastic Gradient Descent as the keras optimizer, which takes the learning rate as input parameter\n",
    "metrics = ['accuracy'] # define (Python) list of metrics\n",
    "\n",
    "network = build_neural_network(data_size_in, n_classes)\n",
    "network.compile(loss=loss, optimizer=sgd, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "\n",
    "If we are using categorical cross-entropy as loss function, we need a representation of labels in the training (and later validation) data in a 'one-hot' form. This means that if we have 5 classes, the format of labels has to be the following:\n",
    "\n",
    "* ```y_train = 1 -> [1, 0, 0, 0, 0]```\n",
    "* ```y_train = 2 -> [0, 1, 0, 0, 0]```\n",
    "* ```y_train = 3 -> [0, 0, 1, 0, 0]```\n",
    "* ```y_train = 4 -> [0, 0, 0, 1, 0]```\n",
    "* ```y_train = 5 -> [0, 0, 0, 0, 1]```\n",
    "\n",
    "Lucky for you, Keras has implemented such a [function](https://keras.io/utils/#to_categorical). We can use that to convert the given format into a 'one-hot' format. First, check the format of labels in your dataset, then check if the function of Keras does what it is supposed to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replace None with your code ###\n",
    "\n",
    "# Load data using the function we defined above\n",
    "train_x, train_y, val_x, val_y, test_x = load_data()\n",
    "\n",
    "train_y_one_hot = keras.utils.to_categorical(train_y, num_classes=10)\n",
    "\n",
    "val_y_one_hot = keras.utils.to_categorical(val_y, num_classes=10)\n",
    "\n",
    "# This should print: (40000, 10)\n",
    "print(train_y_one_hot.shape)\n",
    "# This should print: (10000, 10)\n",
    "print(val_y_one_hot.shape)\n",
    "\n",
    "# check number of samples per class\n",
    "print (np.sum(train_y_one_hot, axis=0))\n",
    "print (np.sum(val_y_one_hot, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check validation performance before training\n",
    "All the main components required to train our network have been defined now.\n",
    "However, we have seen that in order to properly monitor the behaviour of a network during training, we should check the performance (the accuracy) on a separate validation set, and see if it returns something sensible.\n",
    "For this purpose, you can use the function ```network.evaluate``` in Keras, and set the ```batch_size``` to a value that fits in your GPU/CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(val_y_one_hot.shape)\n",
    "scores = network.evaluate(val_x, val_y_one_hot, batch_size=128)\n",
    "val_loss = scores[0]\n",
    "val_acc = scores[1]\n",
    "print ('Initial validation accuracy = {:.2f}%'.format(100.*val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Train your network\n",
    "Now you can define a function that does trains the convnet by updating parameters for each mini-batch. This function will have to inculde the two main steps that we implemented last week:\n",
    "\n",
    "  1. a pass over the training set, to update the parameters\n",
    "  2. a pass over the validation set, to check the performance.\n",
    "  \n",
    "During training/validation, you will have to store the loss and accuracy values, in order to visualize them after each epoch in a plot that shows the learning curves. This is useful to monitor the training procedure.\n",
    "Note that all these steps have been implemented in the previous assignment, you can reuse a lot of that code!\n",
    "\n",
    "During training, we will also be saving to disk the parameters of the network wich has the best performance on the validation set. This will be stored as the file ```best_model.h5``` in the direcotry ```network_dir```, which by default is the root directory of this notebook.\n",
    "\n",
    "We wrap the training code in a function so that you can re-use it of each of the tasks in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replace None with your code ###\n",
    "\n",
    "def train_network(network, x_training, y_training, x_validation, y_validation, n_epoch, batch_size, network_filepath):\n",
    "\n",
    "    # lists where we will be storing values during training, for visualization purposes\n",
    "    tra_losses = [] # list for training loss\n",
    "    tra_accs = [] # list for training accuracy\n",
    "    val_losses = [] # list for validation loss\n",
    "    val_accs = [] # list for validation accuracy\n",
    "\n",
    "    # we want to save the parameters that give the best performance on the validation set\n",
    "    # therefore, we store the best validation accuracy, and save the parameters to disk\n",
    "    best_validation_accuracy = 0 # best validation accuracy\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        st = time.time()\n",
    "\n",
    "        # Train your network\n",
    "        results = network.fit(x_training, y_training, batch_size=batch_size)\n",
    "\n",
    "        # Get training loss and accuracy\n",
    "        training_loss = results.history['loss']\n",
    "        training_accuracy = results.history['acc']\n",
    "\n",
    "        # Add to list\n",
    "        tra_losses.append(training_loss)\n",
    "        tra_accs.append(training_accuracy)\n",
    "\n",
    "        # Evaluate performance (loss and accuracy) on validation set\n",
    "        scores = network.evaluate(x_validation, y_validation, batch_size=batch_size)     \n",
    "        validation_loss = scores[0]\n",
    "        validation_accuracy = scores[1]\n",
    "\n",
    "        # Add to list\n",
    "        val_losses.append(validation_loss)\n",
    "        val_accs.append(validation_accuracy)\n",
    "\n",
    "        # (Possibly) update best validation accuracy and save the network\n",
    "        if validation_accuracy > best_validation_accuracy:\n",
    "            best_validation_accuracy = validation_accuracy\n",
    "            network.save(network_filepath)\n",
    "\n",
    "        # Visualization of the learning curves\n",
    "        fig = plt.figure(figsize=(10, 5))\n",
    "        tra_loss_plt, = plt.plot(range(len(tra_losses)), tra_losses, 'b')\n",
    "        tra_accs_plt, = plt.plot(range(len(tra_accs)), tra_accs, 'c')\n",
    "        val_loss_plt, = plt.plot(range(len(val_losses)), val_losses, 'g')\n",
    "        val_acc_plt, = plt.plot(range(len(val_accs)), val_accs, 'r')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.legend([tra_loss_plt, tra_accs_plt, val_loss_plt, val_acc_plt], \n",
    "                  ['training loss', 'training accuracy', 'validation loss', 'validation accuracy'],\n",
    "                  loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        plt.title('Best validation accuracy = {:.2f}%'.format(100. * best_validation_accuracy))\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        time.sleep(.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Train your network\n",
    "Now that you have defined all the parameters and the functions necessary to train and validate your network, use this cell to run your exepriment. Define a *network_name*, which will be used to (1) save the parameters of the trained network to disk and (2) save a csv file to submit to challenger. Since you will be running several experiments and reusing the same cell (copy-paste) several times, having a name for the network used in each experiment is handy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define batch size.\n",
    "batch_size = 128\n",
    "\n",
    "# keep this low (like 1 or 2) when just writing code, increase it to 350 or higher when\n",
    "# submitting to Cartesius or running it on GPU. \n",
    "\n",
    "n_epoch = 2\n",
    "file_dir = './'\n",
    "network_filepath = os.path.join(file_dir, 'best_model_net_task_2.h5')\n",
    "\n",
    "train_network(network, train_x, train_y_one_hot, val_x, val_y_one_hot, n_epoch, batch_size, network_filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and submit results to grand-challenge\n",
    "Now you can run the network on the test set, get the predicted labels, and submit the results to https://ismi-cifar.grand-challenge.org/introduction/ for evaluation. Use the following code (copy-paste) also for future tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load the model found for the best accuracy; load_model function takes care of compiling again the function \n",
    "best_network=keras.models.load_model(network_filepath)\n",
    "\n",
    "network_name = 'network_task_2'\n",
    "test_prediction = best_network.predict(test_x, batch_size=128)\n",
    "test_y = np.argmax(test_prediction, axis=1)\n",
    "\n",
    "# write csv files with outputs\n",
    "ho = open('./results_{}.csv'.format(network_name), 'w')\n",
    "ho.write('filename, label\\n')\n",
    "for filename, label in zip(batches['cifar10-test']['filenames'], test_y):\n",
    "    ho.write('{}, {}\\n'.format(filename, label))\n",
    "ho.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Add dropout layers\n",
    "Modify your network and add dropout.\n",
    "\n",
    "**Hint**: dropout is typically added to fully-connected layers, but it can be applied to convolutional layers as well.\n",
    "\n",
    "In order to prepare and run your experiment, copy and modify previous cells to fill in the next three cells. Please do the same for the following tasks as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neural_network_drop(data_size_in, n_classes):\n",
    "    \n",
    "        \n",
    "    '''  This network scored x on Grand Challenge (300 epochs, 128 batch size\n",
    "    '''\n",
    "    \n",
    "    inputs = layers.Input(shape=data_size_in, name='input')\n",
    "    conv1 = layers.Conv2D(32, (3,3), activation='relu')(inputs)\n",
    "    max1 = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(conv1)\n",
    "    drop1 = layers.Dropout(0.5)(max1)\n",
    "    conv2 = layers.Conv2D(32, (3,3), activation='relu', padding='same')(drop1)\n",
    "    max2 = layers.MaxPooling2D(pool_size=(3, 3), strides=(3, 3))(conv2)\n",
    "    drop2 = layers.Dropout(0.5)(max2)\n",
    "    fulcon1 = layers.Dense(80, activation='relu')(drop2)\n",
    "    flatten = layers.Flatten()(fulcon1)\n",
    "    softmax = layers.Dense(n_classes, activation='softmax')(flatten)\n",
    "\n",
    "    network = Model(inputs=inputs, output=softmax)\n",
    "    \n",
    "    print(network.summary())\n",
    "    \n",
    "    return network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the network, you can copy code from the first task.\n",
    "\n",
    "network = build_neural_network_drop(data_size_in, n_classes)\n",
    "network.compile(loss=loss, optimizer=sgd, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validate the network\n",
    "network_filepath = os.path.join(file_dir, 'best_model_net_dropout.h5')\n",
    "\n",
    "train_network(network, train_x, train_y_one_hot, val_x, val_y_one_hot, n_epoch, batch_size, network_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question\n",
    "The learning curves changed after adding dropout. How, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* By randomly 'deactivating' nodes the trainer cannot put all its trust in one node and will distribute the weight more evenly. *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and submit results to grand-challenge\n",
    "Now you can run the network on the test set, get the predicted labels, and submit the results to grand-challenge.org for evaluation. Use the following code (copy-paste) also for future tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load the model found for the best accuracy; load_model function takes care of compiling again the function \n",
    "best_network=keras.models.load_model(network_filepath)\n",
    "\n",
    "network_name = 'network_task_3'\n",
    "test_prediction = best_network.predict(test_x, batch_size=128)\n",
    "test_y = np.argmax(test_prediction, axis=1)\n",
    "\n",
    "# write csv files with outputs\n",
    "ho = open('./results_{}.csv'.format(network_name), 'w')\n",
    "ho.write('filename, label\\n')\n",
    "for filename, label in zip(batches['cifar10-test']['filenames'], test_y):\n",
    "    ho.write('{}, {}\\n'.format(filename, label))\n",
    "ho.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Add batch normalization\n",
    "Add batch normalization to your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neural_network_batch_norm(data_size_in, n_classes):\n",
    "    \n",
    "    inputs = layers.Input(shape=data_size_in, name='input')\n",
    "    conv1 = layers.Conv2D(32, (3,3), activation='relu')(inputs)\n",
    "    norm1 = layers.BatchNormalization(axis=1)(conv1)\n",
    "    max1 = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(norm1)\n",
    "    drop1 = layers.Dropout(0.5)(max1)\n",
    "    conv2 = layers.Conv2D(32, (3,3), activation='relu', padding='same')(drop1)\n",
    "    norm2 = layers.BatchNormalization(axis=1)(conv2)\n",
    "    max2 = layers.MaxPooling2D(pool_size=(3, 3), strides=(3, 3))(norm2)\n",
    "    drop2 = layers.Dropout(0.5)(max2)\n",
    "    fulcon1 = layers.Dense(80, activation='relu')(drop2)\n",
    "    flatten = layers.Flatten()(fulcon1)\n",
    "    softmax = layers.Dense(n_classes, activation='softmax')(flatten)\n",
    "\n",
    "    network = Model(inputs=inputs, output=softmax)\n",
    "    \n",
    "    print(network.summary())\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the network, you can copy code from the first task.\n",
    "\n",
    "network = build_neural_network_batch_norm(data_size_in, n_classes)\n",
    "network.compile(loss=loss, optimizer=sgd, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validate the network\n",
    "network_filepath = os.path.join(file_dir, 'best_model_net_batchnorm.h5')\n",
    "n_epoch = 3\n",
    "train_network(network, train_x, train_y_one_hot, val_x, val_y_one_hot, n_epoch, batch_size, network_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and submit results to grand-challenge\n",
    "Now you can run the network on the test set, get the predicted labels, and submit the results to grand-challenge.org for evaluation. Use the following code (copy-paste) also for future tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load the model found for the best accuracy; load_model function takes care of compiling again the function \n",
    "best_network=keras.models.load_model(network_filepath)\n",
    "\n",
    "network_name = 'network_task_4'\n",
    "test_prediction = best_network.predict(test_x, batch_size=128)\n",
    "test_y = np.argmax(test_prediction, axis=1)\n",
    "\n",
    "# write csv files with outputs\n",
    "ho = open('./results_{}.csv'.format(network_name), 'w')\n",
    "ho.write('filename, label\\n')\n",
    "for filename, label in zip(batches['cifar10-test']['filenames'], test_y):\n",
    "    ho.write('{}, {}\\n'.format(filename, label))\n",
    "ho.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Try different initialization strategies\n",
    "Try different Weight initialization strategies in your network. Make a new cell for each different strategy and retrain your network every time. Please see https://keras.io/initializers/ for examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neural_network_diff_init(data_size_in, n_classes, kernal_init, bias_init):\n",
    "    \n",
    "    #inputs = layers.Input(shape=data_size_in)\n",
    "    inputs = layers.Input(shape=data_size_in, name='input_t5')\n",
    "    conv1 = layers.Conv2D(32, (3,3), activation='relu')(inputs)\n",
    "    max1 = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(conv1)\n",
    "    conv2 = layers.Conv2D(32, (3,3), activation='relu', padding='same')(max1)\n",
    "    max2 = layers.MaxPooling2D(pool_size=(3, 3), strides=(3, 3))(conv2)\n",
    "    fulcon1 = layers.Dense(80, activation='relu')(max2)\n",
    "    flatten = layers.Flatten()(fulcon1)\n",
    "    softmax = layers.Dense(n_classes, activation='softmax', kernel_initializer=kernal_init, bias_initializer=bias_init)(flatten)\n",
    "    \n",
    "    model = Model(inputs=inputs, output=softmax)\n",
    "    #model.add(Dense(64, kernel_initializer='random_uniform', bias_initializer='zeros'))\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the network, you can copy code from the first task.\n",
    "networks = [];\n",
    "\n",
    "#function to add network and init string to list as a tuple (network, init_string)\n",
    "def addNetwork(network_array, kernal_init, bias_init):\n",
    "    network_array.append( (build_neural_network_diff_init(data_size_in, n_classes, kernal_init, bias_init), \"kernal: \"+kernal_init+\"   bias: \"+bias_init))\n",
    "\n",
    "addNetwork(networks, 'random_uniform', 'zeros')\n",
    "addNetwork(networks, 'random_normal', 'zeros')\n",
    "addNetwork(networks, 'random_uniform', 'ones')\n",
    "addNetwork(networks, 'random_normal', 'ones')\n",
    "addNetwork(networks, 'orthogonal', 'zeros')\n",
    "addNetwork(networks, 'orthogonal', 'ones')\n",
    "\n",
    "for network in networks:\n",
    "    print(\"Compiling with initializations \\n\"+network[1])\n",
    "    network[0].compile(loss=loss, optimizer=sgd, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validate the network\n",
    "network_filepath = os.path.join(file_dir, 'best_model_net_init.h5')\n",
    "for network in networks:\n",
    "    print(\"Training with initializations \\n\"+network[1])\n",
    "    train_network(network[0], train_x, train_y_one_hot, val_x, val_y_one_hot, n_epoch, batch_size, network_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and submit results to grand-challenge\n",
    "Now you can run the network on the test set, get the predicted labels, and submit the results to grand-challenge.org for evaluation. Use the following code (copy-paste) also for future tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load the model found for the best accuracy; load_model function takes care of compiling again the function \n",
    "best_network=keras.models.load_model(network_filepath)\n",
    "\n",
    "network_name = 'network_task_5'\n",
    "test_prediction = best_network.predict(test_x, batch_size=128)\n",
    "test_y = np.argmax(test_prediction, axis=1)\n",
    "\n",
    "# write csv files with outputs\n",
    "ho = open('./results_{}.csv'.format(network_name), 'w')\n",
    "ho.write('filename, label\\n')\n",
    "for filename, label in zip(batches['cifar10-test']['filenames'], test_y):\n",
    "    ho.write('{}, {}\\n'.format(filename, label))\n",
    "ho.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: Try different nonlinearities\n",
    "Try different nonlinearities in your network. You can view supported activations functions in the Keras documentation: https://keras.io/activations/. If you want you can also try the advanced activations; these are implemented as layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neural_network_diff_nonlin(data_size_in, n_classes):\n",
    " \n",
    "    inputs = layers.Input(shape=data_size_in)\n",
    "        \n",
    "    inputs = layers.Input(shape=data_size_in, name='input')\n",
    "    conv1 = layers.Conv2D(32, (3,3))(inputs)\n",
    "    elu1 = layers.ELU(alpha=1.0)(conv1)\n",
    "    \n",
    "    max1 = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(elu1)\n",
    "    conv2 = layers.Conv2D(32, (3,3), padding='same')(max1)\n",
    "    elu2 = layers.ELU(alpha=1.0)(conv2)\n",
    "    \n",
    "    max2 = layers.MaxPooling2D(pool_size=(3, 3), strides=(3, 3))(elu2)\n",
    "    fulcon1 = layers.Dense(80)(max2)\n",
    "    elu3 = layers.ELU(alpha=1.0)(fulcon1)\n",
    "    \n",
    "    flatten = layers.Flatten()(elu3)\n",
    "    softmax = layers.Dense(n_classes, activation='softmax')(flatten)\n",
    "\n",
    "    network = Model(inputs=inputs, output=softmax)\n",
    "    \n",
    "    print(network.summary())\n",
    "    \n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neural_network_diff_nonlinLeaky(data_size_in, n_classes):\n",
    "    # was given this week, uses Leaky Relu\n",
    "    # Other options that look cool are ELU and Prelu\n",
    "    inputs = layers.Input(shape=data_size_in)\n",
    "    net = layers.Conv2D(filters=16, kernel_size=5)(inputs)\n",
    "    net = layers.LeakyReLU(alpha=0.3)(net)\n",
    "    net = layers.BatchNormalization()(net)\n",
    "    net = layers.MaxPooling2D(pool_size=3)(net)\n",
    "    \n",
    "    net = layers.Conv2D(filters=32, kernel_size=5)(net)\n",
    "    net = layers.LeakyReLU(alpha=0.3)(net)\n",
    "    net = layers.BatchNormalization()(net)\n",
    "    net = layers.MaxPooling2D(pool_size=3)(net)\n",
    "    \n",
    "    net = layers.Flatten()(net)\n",
    "    net = layers.Dense(units=256)(net)\n",
    "    net = layers.LeakyReLU(alpha=0.3)(net)\n",
    "    net = layers.BatchNormalization()(net)\n",
    "    \n",
    "    net = layers.Dropout(rate=0.3)(net)\n",
    "    net = layers.Dense(units=n_classes, activation='softmax')(net)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=net)\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the network, you can copy code from the first task.\n",
    "network = build_neural_network_diff_nonlin(data_size_in, n_classes)\n",
    "# network = build_neural_network_diff_nonlinLeaky(data_size_in, n_classes)\n",
    "network.compile(loss=loss, optimizer=sgd, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validate the network\n",
    "network_filepath = os.path.join(file_dir, 'best_model_net_nonlin.h5')\n",
    "\n",
    "train_network(network, train_x, train_y_one_hot, val_x, val_y_one_hot, n_epoch, batch_size, network_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question\n",
    "Calculating the sigmoid function is computationally expensive. What is the other main weakness of the function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and submit results to grand-challenge\n",
    "Now you can run the network on the test set, get the predicted labels, and submit the results to grand-challenge.org for evaluation. Use the following code (copy-paste) also for future tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load the model found for the best accuracy; load_model function takes care of compiling again the function \n",
    "best_network=keras.models.load_model(network_filepath)\n",
    "\n",
    "network_name = 'network_task_6'\n",
    "test_prediction = best_network.predict(test_x, batch_size=128)\n",
    "test_y = np.argmax(test_prediction, axis=1)\n",
    "\n",
    "# write csv files with outputs\n",
    "ho = open('./results_{}.csv'.format(network_name), 'w')\n",
    "ho.write('filename, label\\n')\n",
    "for filename, label in zip(batches['cifar10-test']['filenames'], test_y):\n",
    "    ho.write('{}, {}\\n'.format(filename, label))\n",
    "ho.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7: L2 normalization\n",
    "Add L2 regularization to your loss calculation. You can find examples at https://keras.io/regularizers/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "def build_neural_network_l2(data_size_in, n_classes):\n",
    "    \n",
    "    \n",
    "    '''  This network scored x% (rank y at the time) on Grand Challenge (300 epochs, 128 batch size\n",
    "    '''\n",
    "        \n",
    "    inputs = layers.Input(shape=data_size_in, name='input')\n",
    "    conv1 = layers.Conv2D(32, (3,3), activation='relu', kernel_regularizer=regularizers.l2(0.01))(inputs)\n",
    "    max1 = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(conv1)\n",
    "    conv2 = layers.Conv2D(32, (3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(max1)\n",
    "    max2 = layers.MaxPooling2D(pool_size=(3, 3), strides=(3, 3))(conv2)\n",
    "    fulcon1 = layers.Dense(80, activation='relu', kernel_regularizer=regularizers.l2(0.01))(max2)\n",
    "    flatten = layers.Flatten()(fulcon1)\n",
    "    softmax = layers.Dense(n_classes, activation='softmax')(flatten)\n",
    "\n",
    "    network = Model(inputs=inputs, output=softmax)\n",
    "    \n",
    "    print(network.summary())\n",
    "    return network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the network, you can copy code from the first task.\n",
    "\n",
    "network = build_neural_network_l2(data_size_in, n_classes)\n",
    "network.compile(loss=loss, optimizer=sgd, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validate the network\n",
    "network_filepath = os.path.join(file_dir, 'best_model_net_l2.h5')\n",
    "\n",
    "train_network(network, train_x, train_y_one_hot, val_x, val_y_one_hot, n_epoch, batch_size, network_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question\n",
    "Is it necessary to monitor the L2 loss during training? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and submit results to grand-challenge\n",
    "Now you can run the network on the test set, get the predicted labels, and submit the results to grand-challenge.org for evaluation. Use the following code (copy-paste) also for future tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load the model found for the best accuracy; load_model function takes care of compiling again the function \n",
    "best_network=keras.models.load_model(network_filepath)\n",
    "\n",
    "network_name = 'network_task_7'\n",
    "test_prediction = best_network.predict(test_x, batch_size=128)\n",
    "test_y = np.argmax(test_prediction, axis=1)\n",
    "\n",
    "# write csv files with outputs\n",
    "ho = open('./results_{}.csv'.format(network_name), 'w')\n",
    "ho.write('filename, label\\n')\n",
    "for filename, label in zip(batches['cifar10-test']['filenames'], test_y):\n",
    "    ho.write('{}, {}\\n'.format(filename, label))\n",
    "ho.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8: Add data augmentation\n",
    "\n",
    "In this task you will have to implement different data augmentation techniques. We have created a simple framework to help you with this. You will have to implement at least three functions that augment an image using some method. Think about the task and images, and decide which augmentations could help the network to generalize.\n",
    "\n",
    "You are free to use data augmentation libraries/frameworks (for example https://pillow.readthedocs.io or https://github.com/mdbloice/Augmentor). Feel free to change the augmentation framework below if you have any other ideas to augment the data or want to use an additional framework. Make sure to document which external software packages you use (if any). If you implement the augmentations yourself **you can get up to 10 bonus points** for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create (at least) three functions that take an image as input and return an augmented image\n",
    "# Please give them sensible names, describing the augmentation.\n",
    "def augmentation_func_1(image):\n",
    "    return image\n",
    "\n",
    "def augmentation_func_2(image):\n",
    "    return image\n",
    "\n",
    "def augmentation_func_3(image):\n",
    "    return image\n",
    "\n",
    "# Fill this array with your augmentation functions\n",
    "augmentations = [augmentation_func_1, augmentation_func_1, augmentation_func_1]\n",
    "\n",
    "# Preview your augmentations on an image\n",
    "test_image = train_x[1].copy()\n",
    "fig, axarr = plt.subplots(ncols=len(augmentations) + 1, nrows=1)\n",
    "axarr[0].imshow((test_image))\n",
    "for key, augment_func in enumerate(augmentations):\n",
    "    axarr[key + 1].imshow(augment_func(test_image))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is my idea (jeff). The below function uses Keras' own augmentation and is way easier...\n",
    "# but has to be called differently because this thing will loop indefinitely\n",
    "# so this really makes things a tad more complex for debugging with Cartisius involved.\n",
    "# https://keras.io/preprocessing/image/\n",
    "# see above for details\n",
    "datagen = preprocessing.image.ImageDataGenerator(rotation_range=90,  #rotate images 90 degrees \n",
    "    width_shift_range=0.2,    # Shift iages to left and right\n",
    "    height_shift_range=0.2,  # Shift images up and down\n",
    "    horizontal_flip=True,  # flip horizontally\n",
    "    vertical_flip=False)  # flip vertically\n",
    "dategen.fit(x_training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below takes the original data as input and returns an augmented dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to augment data\n",
    "# The ratio defines how many of the images will be augmented.\n",
    "def augment_data(original_data, ratio=0.2):\n",
    "\n",
    "    data = original_data.copy()\n",
    "    \n",
    "    # Apply augmentations on a random portion of the data\n",
    "    for augment_func in augmentations:\n",
    "        indices = np.random.choice(len(data), int(len(data) * ratio))\n",
    "    \n",
    "        for i in indices:\n",
    "            data[i] = augment_func(data[i])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our train function needs to be adapted to work with the augmentations. Think about how you can do this. When do you need to apply the augmentations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network_with_augmentations(network, x_training, y_training, x_validation, y_validation, n_epoch, batch_size, network_filepath):\n",
    "    \n",
    "    # Copy your implementation of the previous task and extend it to support augmented images.\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "network_filepath = os.path.join(file_dir, 'best_model_net_augmentations.h5')\n",
    "\n",
    "train_network(network, train_x, train_y_one_hot, val_x, val_y_one_hot, n_epoch, batch_size, network_filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and submit results to grand-challenge\n",
    "Now you can run the network on the test set, get the predicted labels, and submit the results to grand-challenge.org for evaluation. Use the following code (copy-paste) also for future tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load the model found for the best accuracy; load_model function takes care of compiling again the function \n",
    "best_network=keras.models.load_model(network_filepath)\n",
    "\n",
    "network_name = 'network_task_8'\n",
    "test_prediction = best_network.predict(test_x, batch_size=128)\n",
    "test_y = np.argmax(test_prediction, axis=1)\n",
    "\n",
    "# write csv files with outputs\n",
    "ho = open('./results_{}.csv'.format(network_name), 'w')\n",
    "ho.write('filename, label\\n')\n",
    "for filename, label in zip(batches['cifar10-test']['filenames'], test_y):\n",
    "    ho.write('{}, {}\\n'.format(filename, label))\n",
    "ho.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question\n",
    "Should the upside-down flipping be used as augmentation? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 9: Try different architectures\n",
    "\n",
    "The architectures we used so far were pretty simple. Try to build a different (deeper) architecture and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neural_network_deep_arch(data_size_in, n_classes):\n",
    "    #inputs = layers.Input(shape=data_size_in)\n",
    "    inputs = layers.Input(shape=data_size_in, name='input_t9')\n",
    "    conv1 = layers.Conv2D(32, (3,3), activation='relu')(inputs)\n",
    "    #norm1 = layers.BatchNormalization(axis=1)(conv1)             #batch normalisation\n",
    "    #max1  = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(norm1)\n",
    "    drop1 = layers.Dropout(0.33)(conv1)                            #dropout\n",
    "    \n",
    "    conv2 = layers.Conv2D(32, (3,3), activation='relu', padding='same')(drop1)\n",
    "    #norm2 = layers.BatchNormalization(axis=1)(conv2)             #batch normalisation\n",
    "    #max2  = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(norm2)\n",
    "    drop2 = layers.Dropout(0.33)(conv2)                            #dropout\n",
    "    \n",
    "    conv3 = layers.Conv2D(32, (3,3), activation='relu', padding='same')(drop2)\n",
    "    norm3 = layers.BatchNormalization(axis=1)(conv3)             #batch normalisation\n",
    "    max3  = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(norm3)\n",
    "    drop3 = layers.Dropout(0.33)(max3)                            #dropout\n",
    "    \n",
    "    conv4 = layers.Conv2D(32, (3,3), activation='relu', kernel_regularizer=regularizers.l2(0.01))(drop3) #L2 normalisation\n",
    "    #norm4 = layers.BatchNormalization(axis=1)(conv4)             #batch normalisation\n",
    "    #max4  = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(norm4)\n",
    "    drop4 = layers.Dropout(0.33)(conv4)                            #dropout\n",
    "    \n",
    "    conv5 = layers.Conv2D(32, (3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(drop4) #L2 normalisation\n",
    "    #max5  = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(conv5)\n",
    "    drop5 = layers.Dropout(0.33)(conv5)                            #dropout\n",
    "    \n",
    "    #Add non lineairities\n",
    "    conv6 = layers.Conv2D(32, (3,3))(drop5)\n",
    "    #elu6 = layers.ELU(alpha=1.0)(conv6)\n",
    "    #max6 = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(elu6)\n",
    "    drop6 = layers.Dropout(0.33)(conv6)                            #dropout\n",
    "    \n",
    "    conv7 = layers.Conv2D(32, (3,3), padding='same')(drop6)\n",
    "    elu7 = layers.ELU(alpha=1.0)(conv7)\n",
    "    #max7 = layers.MaxPooling2D(pool_size=(3, 3), strides=(3, 3))(elu7)\n",
    "    drop7 = layers.Dropout(0.33)(elu7)                            #dropout\n",
    "    \n",
    "    fulcon8 = layers.Dense(80)(drop7)\n",
    "    elu8 = layers.ELU(alpha=1.0)(fulcon8)\n",
    "    \n",
    "    #max3 = layers.MaxPooling2D(pool_size=(3, 3), strides=(3, 3))(conv3)\n",
    "    fulcon2 = layers.Dense(80, activation='relu')(elu8)\n",
    "    flatten = layers.Flatten()(fulcon2)\n",
    "    softmax = layers.Dense(n_classes, activation='softmax')(flatten)\n",
    "    \n",
    "    model = Model(inputs=inputs, output=softmax)\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neural_network_deep_arch_two(data_size_in, n_classes):\n",
    "    # based on this Convnet paper: https://arxiv.org/pdf/1412.6806.pdf from 2014 (published in 2015?) by Springenberg et al\n",
    "    # titled \"Striving for simplicity, the all-convolutional net\"\n",
    "    # and the reported 84-91% result \n",
    "    # Do we get bonusses for two epicly deep networks? :D\n",
    "    # I \"deeply\" suspect ELU would be a better choice than Relu here\n",
    "    inputs = layers.Input(shape=data_size_in, name='input_t9')\n",
    "    x = layers.Conv2D(96, (3,3),activation='relu', padding = 'same')(inputs)\n",
    "    x = layers.Dropout(0.2)(x)                            #dropout\n",
    "    x = layers.Conv2D(96, (3,3),activation='relu', padding = 'same')(x)\n",
    "    x = layers.Conv2D(96, (3,3),activation='relu', padding = 'same', strides=2)(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Conv2D(192, (3, 3), activation='relu', padding = 'same')(x)\n",
    "    x = layers.Conv2D(192, (3, 3), activation='relu', padding = 'same')(x)\n",
    "    x = layers.Conv2D(192, (3, 3), activation='relu', padding = 'same', strides=2)(x)\n",
    "    x = layers.Dropout(0.5)(x)    \n",
    "    x = layers.Conv2D(192, (3, 3), activation='relu', padding = 'same')(x)    \n",
    "    x = layers.Conv2D(192, (1, 1), activation='relu', padding='valid')(x)\n",
    "    x = layers.Conv2D(10, (1, 1), activation='softmax', padding='valid')(x)\n",
    "    gpl = layers.GlobalAveragePooling2D()(x)\n",
    "    network = Model(inputs=inputs, output=gpl)\n",
    "    \n",
    "    print(network.summary())\n",
    "    \n",
    "    \n",
    "    return network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_neural_network_deep_arch_two' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-dcd223ae92d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Compile the network, you can copy code from the first task.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnetwork\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_neural_network_deep_arch_two\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_size_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'build_neural_network_deep_arch_two' is not defined"
     ]
    }
   ],
   "source": [
    "# Compile the network, you can copy code from the first task.\n",
    "\n",
    "network = build_neural_network_deep_arch(data_size_in, n_classes)\n",
    "network = build_neural_network_deep_arch_two(data_size_in, n_classes)\n",
    "network.compile(loss=loss, optimizer=sgd, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train and validate the network\n",
    "network_filepath = os.path.join(file_dir, 'best_model_net_deep.h5')\n",
    "\n",
    "train_network(network, train_x, train_y_one_hot, val_x, val_y_one_hot, n_epoch, batch_size, network_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and submit results to grand-challenge\n",
    "Now you can run the network on the test set, get the predicted labels, and submit the results to grand-challenge.org for evaluation. Use the following code (copy-paste) also for future tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load the model found for the best accuracy; load_model function takes care of compiling again the function \n",
    "best_network=keras.models.load_model(network_filepath)\n",
    "\n",
    "network_name = 'network_task_9'\n",
    "test_prediction = best_network.predict(test_x, batch_size=128)\n",
    "test_y = np.argmax(test_prediction, axis=1)\n",
    "\n",
    "# write csv files with outputs\n",
    "ho = open('./results_{}.csv'.format(network_name), 'w')\n",
    "ho.write('filename, label\\n')\n",
    "for filename, label in zip(batches['cifar10-test']['filenames'], test_y):\n",
    "    ho.write('{}, {}\\n'.format(filename, label))\n",
    "ho.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 10 (optional): Tools to monitor training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> add your code here <<<"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
